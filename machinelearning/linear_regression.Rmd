---
title: "Simple Linear Regression"
author: "Adam Caffrey"
date: "04/01/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(tidymodels)
```

Linear regression is the simplest machine learning concept one should know. On this page, I use linear regression to estimate the length of petals using the famous iris dataset and in doing so will layout the typical machine learning workflow. 

```{r}
data <- iris
```

The data consists of 5 columns, 4 numeric and 1 nominal. For this example I shall create a linear model to predict the petal length from the petal width. No data is missing and no cleaning is necessary for this dataset.

```{r fig.align="center"}
glimpse(data)
```

Data is first split into a testing and training set using the rsample package: 

```{r}
data_split <- initial_split(data, prop = 3/4)
data_train <- training(data_split)
data_test <- testing(data_split)
```

A linear model is selected from the **lm** library...

```{r}
lr_model <- 
  linear_reg() %>% 
  set_engine("lm")
```

...and a very simple recipe is produced.

```{r}
lr_recipe <- recipe(Petal.Length ~ Petal.Width, data_train)
```

A workflow object is used to combine the model and recipe. This also removes the need to **prep** and **bake** the recipe:

```{r}
lr_wf <- 
  workflow() %>% 
  add_model(lr_model) %>% 
  add_recipe(lr_recipe)
```

The **fit** method then uses the training data to establish a model. The **lr_fit** object contains the model coefficients. 

```{r}
(lr_fit <- fit(lr_wf, data_train))
```

Using this model we can now predict the length of petals using the test data. 

```{r fig.align="center"}
(pred <- predict(lr_fit, data_test) %>% 
  select(.pred) %>% 
  bind_cols(., "truth" = data_test$Petal.Length, 
            "resid" = data_test$Petal.Length - .$.pred))
```

The [performance](./model_performance.html) of linear models are typically graded using rmse, rsq and mae values. These are obtained using the **metrics** method. 

```{r fig.align="center"}
pred %>%  metrics(truth = truth, estimate = .pred)
```

A more intuitive visualisation can be obtained by plotting the predicted values against the actual values [left]. The diagonal line has a slope of 1 indicating an exact match. A good match is found between predicted and actual values. Additionally, linear regression requires homoscedasticity (constant variance amongst errors). This behaviour is displayed below [right] though few data points are available to be conclusive.

```{r fig.align="center", fig.width=8, fig.height=4}
ggplt1 <- 
  ggplot(data = pred, aes(x = .pred, y = truth)) +
    geom_point() +
    geom_abline(slope = 1, col = "red") +
    theme_linedraw() +
    labs(x = "Predicted Value", y = "Actual Value") +
    lims(x = c(0,9), y = c(0,8))

ggplt2 <- 
  ggplot(data = pred, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_abline(slope = 0, intercept = 0, col = "red") +
    theme_linedraw() +
    labs(x = "Predicted Value", y = "Residuals") +
    lims(x = c(0,9), y = c(-2,2))

gridExtra::grid.arrange(ggplt1, ggplt2, nrow = 1)

```

This simple introduction to linear regression highlights the following tidymodels concepts:   
1. workflow objects   
2. recipe creation   
3. model selection   
4. data fitting   
5. data prediction
6. model evaluation     







#